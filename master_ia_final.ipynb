{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaelleCornec/random_teachers/blob/main/master_ia_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation du notebook"
      ],
      "metadata": {
        "id": "pBZYk6tR7dqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il faut bien penser à activer le GPU pour aller plus vite"
      ],
      "metadata": {
        "id": "BPs6IAv77vZi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pAZPsNPK_1YJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "### SETTINGS\n",
        "##########################\n",
        "\n",
        "# Hyperparameters\n",
        "RANDOM_SEED = 1\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "# changer ça en fonction des expériences:\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Architecture\n",
        "NUM_FEATURES = 28*28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Other\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "GRAYSCALE = False\n",
        "\n",
        "print(f\"Device utilisé: {DEVICE}\")"
      ],
      "metadata": {
        "id": "DBZq3tQ2Ad5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f5c8b2-5917-493c-907c-1939450946cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device utilisé: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "### MNIST DATASET\n",
        "##########################\n",
        "\n",
        "# Note transforms.ToTensor() scales input images\n",
        "# to 0-1 range\n",
        "train_dataset = datasets.CIFAR10(root='data',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='data',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False)\n",
        "\n",
        "# Checking the dataset\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL7i7KVAAnDo",
        "outputId": "721efc1b-0566-4d58-a01f-41850e1a5af2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:09<00:00, 18688815.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Image batch dimensions: torch.Size([128, 3, 32, 32])\n",
            "Image label dimensions: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On vérifie que les batchs sont bien initialisés"
      ],
      "metadata": {
        "id": "Apoq7C087n5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(DEVICE)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for epoch in range(2):\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        print('Epoch:', epoch+1, end='')\n",
        "        print(' | Batch index:', batch_idx, end='')\n",
        "        print(' | Batch size:', y.size()[0])\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlPzwhyMB_so",
        "outputId": "929f786e-7996-46be-d95d-5d49ea02e0f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Batch index: 0 | Batch size: 128\n",
            "Epoch: 2 | Batch index: 0 | Batch size: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (features, targets) in enumerate(test_loader):\n",
        "\n",
        "    features = features\n",
        "    targets = targets\n",
        "    break\n",
        "\n",
        "nhwc_img = np.transpose(features[1], axes=(1, 2, 0))\n",
        "nhw_img = nhwc_img.numpy()\n",
        "plt.imshow(nhw_img)\n",
        "print(targets[1].item())\n"
      ],
      "metadata": {
        "id": "9H4rptPoJP9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "a9831df0-fef3-4e0b-97ff-a7612248c7e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition des modèles"
      ],
      "metadata": {
        "id": "_bGb4hMw8eH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet18"
      ],
      "metadata": {
        "id": "MV_aZXa_8xnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes, grayscale):\n",
        "        self.inplanes = 64\n",
        "        if grayscale:\n",
        "            in_dim = 1\n",
        "        else:\n",
        "            in_dim = 3\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, (2. / n)**.5)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        # because MNIST is already 1x1 here:\n",
        "        # disable avg pooling\n",
        "        #x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        logits = self.fc(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas\n",
        "\n",
        "\n",
        "\n",
        "def resnet18(num_classes):\n",
        "    \"\"\"Constructs a ResNet-18 model.\"\"\"\n",
        "    model = ResNet(block=BasicBlock,\n",
        "                   layers=[2, 2, 2, 2],\n",
        "                   num_classes=NUM_CLASSES,\n",
        "                   grayscale=GRAYSCALE)\n",
        "    return model"
      ],
      "metadata": {
        "id": "U9FyYpqsCmBk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG"
      ],
      "metadata": {
        "id": "Oa4DBzGP85Kw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "à rajouter"
      ],
      "metadata": {
        "id": "jbt7ZF2F87Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrainement des modèles"
      ],
      "metadata": {
        "id": "GtqQbNDH9C4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        logits, probas = model(features)\n",
        "        _, predicted_labels = torch.max(probas, 1)\n",
        "        num_examples += targets.size(0)\n",
        "        correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "id": "PKTiLY2GqXa6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_student_parameters(teacher, student, alpha):\n",
        "    alpha_tensor = torch.tensor(alpha, dtype=torch.float32)  # Convert alpha to a tensor\n",
        "    delta = torch.sqrt(alpha_tensor**2 + (1 - alpha_tensor)**2)\n",
        "    for param_teacher, param_student in zip(teacher.parameters(), student.parameters()):\n",
        "        fresh_init = torch.randn_like(param_student.data)  # Fresh initialization\n",
        "        new_param_student = (1 - alpha_tensor) * param_teacher.data + alpha_tensor * fresh_init\n",
        "        new_param_student /= delta\n",
        "        param_student.data.copy_(new_param_student)\n"
      ],
      "metadata": {
        "id": "5LzDbWV5SHeV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement non-naif du teacher (ne reproduit pas une figure ou résultat du papier)"
      ],
      "metadata": {
        "id": "s1Ah_uPp31Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On entraine une première fois un modèle, histoire de vérifier que tout fonctionne. Il jouera le rôle de teacher, dans l'idée il faudra initialiser le teacher aléatoirement"
      ],
      "metadata": {
        "id": "N5mMDxeeEu_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_algo():\n",
        "    torch.manual_seed(RANDOM_SEED +1)\n",
        "\n",
        "    model = resnet18(NUM_CLASSES)\n",
        "    model.to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1):\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "            features = features.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            ### FORWARD AND BACK PROP\n",
        "            logits, probas = model(features)\n",
        "            cost = F.cross_entropy(logits, targets)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            cost.backward()\n",
        "\n",
        "            ### UPDATE MODEL PARAMETERS\n",
        "            optimizer.step()\n",
        "\n",
        "            ### LOGGING\n",
        "            if not batch_idx % 50:\n",
        "                print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
        "                      %(epoch+1, NUM_EPOCHS, batch_idx,\n",
        "                        len(train_loader), cost))\n",
        "\n",
        "        model.eval()\n",
        "        with torch.set_grad_enabled(False): # save memory during inference\n",
        "            print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
        "                  epoch+1, NUM_EPOCHS,\n",
        "                  compute_accuracy(model, train_loader, device=DEVICE)))\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "#test_algo()"
      ],
      "metadata": {
        "id": "NvjC8htlDGYd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproduction Fig.1"
      ],
      "metadata": {
        "id": "MQLy105r4ta0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proposition distance : MSE"
      ],
      "metadata": {
        "id": "vqquwaRH5fKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le teacher n'est pas randomisé à chaque epoch, pas de linear probing non plus"
      ],
      "metadata": {
        "id": "Bof5OpNv58Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_knowledge_distillation_mse(teacher, student, train_loader, epochs, learning_rate, device):\n",
        "    optimizer = torch.optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()  # L'enseignant ne va pas être actualisé\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      student.train() # On va actualiser l'étudiant\n",
        "\n",
        "      for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "          features = features.to(DEVICE)\n",
        "          targets = targets.to(DEVICE)\n",
        "\n",
        "          ## Forward pass avec l'enseignant et l'étudiant\n",
        "          with torch.no_grad():\n",
        "              teacher_logits, _ = teacher(features)\n",
        "          student_logits, _ = student(features)\n",
        "\n",
        "          # La distance est calculée par Mean Squared Error (pas défini dans l'article)\n",
        "          loss_distillation = F.mse_loss(student_logits, teacher_logits)\n",
        "          # si on veut définir une loss plus complexe, il suffirait de la plug-in ci-dessous\n",
        "          loss = loss_distillation\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          ## Backpropagation\n",
        "          loss.backward()\n",
        "\n",
        "          ## Update les paramètres du modèle\n",
        "          optimizer.step()\n",
        "\n",
        "          ## Log des résultats\n",
        "          if not batch_idx % 50:\n",
        "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
        "                   %(epoch+1, epochs, batch_idx,\n",
        "                    len(train_loader), loss))\n",
        "      student.eval()\n",
        "      with torch.set_grad_enabled(False): # save memory during inference\n",
        "          print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
        "                epoch+1, epochs,\n",
        "                compute_accuracy(student, train_loader, device=device)))\n",
        "\n",
        "      print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
      ],
      "metadata": {
        "id": "8ykXJwREL8Ut"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = resnet18(NUM_CLASSES)\n",
        "teacher.to(DEVICE)\n",
        "student = resnet18(NUM_CLASSES)\n",
        "student.to(DEVICE)\n",
        "\n",
        "print(\"ok\")\n",
        "\n",
        "# train_knowledge_distillation_mse(teacher, student, train_loader, NUM_EPOCHS, LEARNING_RATE, DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDWTMbKlyQ-4",
        "outputId": "b6fe8318-737e-4d38-ff4f-fbae7b8cc293"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# student.eval()\n",
        "# with torch.set_grad_enabled(False):\n",
        "#   print(\"%.3f% %\" % compute_accuracy(teacher, train_loader, device=device))"
      ],
      "metadata": {
        "id": "N50t7M8gMCIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proposition distance: KL divergence"
      ],
      "metadata": {
        "id": "3Qbec2uX6EBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pas (encore) de teacher randomisé à chaque epoch, KL divergence et linear probing"
      ],
      "metadata": {
        "id": "pM4MYx4q6MRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_knowledge_distillation_kl_linear_probing_LR(model,\n",
        "                                                   train_loader,\n",
        "                                                   epochs,\n",
        "                                                   learning_rate,\n",
        "                                                   device,\n",
        "                                                    alpha,\n",
        "                                                   epoch_randomized_teacher = False,\n",
        "                                                  ):\n",
        "    ### Definition des modèles initiaux\n",
        "    teacher = model(NUM_CLASSES)\n",
        "    teacher.to(DEVICE)\n",
        "    student = model(NUM_CLASSES)\n",
        "    student.to(DEVICE)\n",
        "\n",
        "    update_student_parameters(teacher, student, alpha=alpha)\n",
        "\n",
        "    # Definition des variables/comportement des objets\n",
        "    optimizer = torch.optim.Adam(student.parameters(), lr=learning_rate)\n",
        "    teacher.eval()\n",
        "    start_time = time.time()\n",
        "    logistic_regression_kwarg = {\n",
        "        \"max_iter\" : 1000,\n",
        "        \"random_state\" : 0,\n",
        "    }\n",
        "\n",
        "    ### Definition loggers\n",
        "    teacher_accuracy_log = []\n",
        "    student_accuracy_log = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        if epoch_randomized_teacher:\n",
        "            teacher = model(NUM_CLASSES)\n",
        "            teacher.to(DEVICE)\n",
        "            teacher.eval()\n",
        "\n",
        "        student.train()\n",
        "        teacher_logits_list = []\n",
        "        student_logits_list = []\n",
        "        targets_list = []\n",
        "\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits, _ = teacher(features)\n",
        "            student_logits, _ = student(features)\n",
        "\n",
        "            teacher_logits_list.append(teacher_logits.cpu().detach().numpy())\n",
        "            student_logits_list.append(student_logits.cpu().detach().numpy())\n",
        "            targets_list.append(targets.cpu().detach().numpy())\n",
        "\n",
        "            loss_distillation = F.kl_div(F.log_softmax(student_logits, dim=1),\n",
        "                                          F.softmax(teacher_logits, dim=1),\n",
        "                                          reduction='batchmean')\n",
        "            loss = loss_distillation\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if not batch_idx % 50:\n",
        "                print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
        "                       %(epoch+1, epochs, batch_idx,\n",
        "                        len(train_loader), loss.item()))\n",
        "\n",
        "        # On convertit les tenseurs pour permettre de concaténer les batchs\n",
        "        teacher_logits_tensor = torch.from_numpy(np.concatenate(teacher_logits_list, axis=0))\n",
        "        student_logits_tensor = torch.from_numpy(np.concatenate(student_logits_list, axis=0))\n",
        "        targets_tensor = torch.from_numpy(np.concatenate(targets_list, axis=0))\n",
        "\n",
        "        # Ensuite on fit la logistic regression sur les logits du teacher\n",
        "        logistic_regression_teacher = LogisticRegression(**logistic_regression_kwarg)\n",
        "        logistic_regression_teacher.fit(teacher_logits_tensor, targets_tensor)\n",
        "        teacher_accuracy = accuracy_score(targets_tensor, logistic_regression_teacher.predict(teacher_logits_tensor))\n",
        "        print(f'Epoch: {epoch+1}/{epochs} | Teacher Logistic Regression Accuracy: {teacher_accuracy:.3f}')\n",
        "\n",
        "        # Ensuite on fit la logistic regression sur les logits du student\n",
        "        logistic_regression_student = LogisticRegression(**logistic_regression_kwarg)\n",
        "        logistic_regression_student.fit(student_logits_tensor, targets_tensor)\n",
        "        student_accuracy = accuracy_score(targets_tensor, logistic_regression_student.predict(student_logits_tensor))\n",
        "        print(f'Epoch: {epoch+1}/{epochs} | Student Logistic Regression Accuracy: {student_accuracy:.3f}')\n",
        "        student_accuracy_log.append(student_accuracy)\n",
        "        teacher_accuracy_log.append(teacher_accuracy)\n",
        "        # Evaluate student\n",
        "        student.eval()\n",
        "        with torch.set_grad_enabled(False):\n",
        "            print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
        "                    epoch+1, epochs,\n",
        "                    compute_accuracy(student, train_loader, device=device)))\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "    return teacher_accuracy_log, student_accuracy_log"
      ],
      "metadata": {
        "id": "90m8yGBiA5Un"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ok\")\n",
        "teacher_accuracy_log, student_accuracy_log = train_knowledge_distillation_kl_linear_probing_LR(resnet18,\n",
        "                                                                                            train_loader,\n",
        "                                                                                            NUM_EPOCHS,                                                                                     LEARNING_RATE,\n",
        "                                                                                            DEVICE,\n",
        "                                                                                            alpha = 0.1,\n",
        "                                                                                            epoch_randomized_teacher = True,\n",
        ")"
      ],
      "metadata": {
        "id": "Hoznb-ymxj4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da2ba94-2403-48ef-c875-12155e56656a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n",
            "Epoch: 001/020 | Batch 0000/0391 | Cost: 2.6653\n",
            "Epoch: 001/020 | Batch 0050/0391 | Cost: 0.1520\n",
            "Epoch: 001/020 | Batch 0100/0391 | Cost: 0.0280\n",
            "Epoch: 001/020 | Batch 0150/0391 | Cost: 0.0162\n",
            "Epoch: 001/020 | Batch 0200/0391 | Cost: 0.0081\n",
            "Epoch: 001/020 | Batch 0250/0391 | Cost: 0.0080\n",
            "Epoch: 001/020 | Batch 0300/0391 | Cost: 0.0052\n",
            "Epoch: 001/020 | Batch 0350/0391 | Cost: 0.0049\n",
            "Epoch: 1/20 | Teacher Logistic Regression Accuracy: 0.245\n",
            "Epoch: 1/20 | Student Logistic Regression Accuracy: 0.142\n",
            "Epoch: 001/020 | Train: 9.764%\n",
            "Time elapsed: 17.50 min\n",
            "Epoch: 002/020 | Batch 0000/0391 | Cost: 0.0228\n",
            "Epoch: 002/020 | Batch 0050/0391 | Cost: 0.0050\n",
            "Epoch: 002/020 | Batch 0100/0391 | Cost: 0.0035\n",
            "Epoch: 002/020 | Batch 0150/0391 | Cost: 0.0025\n",
            "Epoch: 002/020 | Batch 0200/0391 | Cost: 0.0017\n",
            "Epoch: 002/020 | Batch 0250/0391 | Cost: 0.0027\n",
            "Epoch: 002/020 | Batch 0300/0391 | Cost: 0.0021\n",
            "Epoch: 002/020 | Batch 0350/0391 | Cost: 0.0018\n",
            "Epoch: 2/20 | Teacher Logistic Regression Accuracy: 0.235\n",
            "Epoch: 2/20 | Student Logistic Regression Accuracy: 0.165\n",
            "Epoch: 002/020 | Train: 10.082%\n",
            "Time elapsed: 35.06 min\n",
            "Epoch: 003/020 | Batch 0000/0391 | Cost: 0.0148\n",
            "Epoch: 003/020 | Batch 0050/0391 | Cost: 0.0014\n",
            "Epoch: 003/020 | Batch 0100/0391 | Cost: 0.0011\n",
            "Epoch: 003/020 | Batch 0150/0391 | Cost: 0.0011\n",
            "Epoch: 003/020 | Batch 0200/0391 | Cost: 0.0013\n",
            "Epoch: 003/020 | Batch 0250/0391 | Cost: 0.0011\n",
            "Epoch: 003/020 | Batch 0300/0391 | Cost: 0.0009\n",
            "Epoch: 003/020 | Batch 0350/0391 | Cost: 0.0011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_accuracy_log, student_accuracy_log"
      ],
      "metadata": {
        "id": "-0DG_LrKxph2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR sur données complètes"
      ],
      "metadata": {
        "id": "w-YYEN5KpVw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_loader(loader):\n",
        "    features_list = []\n",
        "    targets_list = []\n",
        "    for batch_features, batch_targets in loader:\n",
        "        features_list.append(batch_features.view(batch_features.size(0), -1))\n",
        "        targets_list.append(batch_targets)\n",
        "    return torch.cat(features_list, dim=0), torch.cat(targets_list, dim=0)\n",
        "\n",
        "# Flatten the train_loader\n",
        "flattened_features, flattened_targets = flatten_loader(train_loader)\n",
        "\n",
        "# Fit logistic regression on flattened data\n",
        "logistic_regression = LogisticRegression(max_iter=100, random_state=0)\n",
        "logistic_regression.fit(flattened_features.numpy(), flattened_targets.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "OX-5SJ8lfzSs",
        "outputId": "e11d3fcf-1567-403f-d3c1-477bea8545af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_features, flattened_targets = flatten_loader(test_loader)\n",
        "accuracy_score(flattened_targets, logistic_regression.predict(flattened_features))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_VRSsC7q2DK",
        "outputId": "8738630f-8b32-4105-a171-3bb41f70812e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4051"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autre expérience (à compléter)"
      ],
      "metadata": {
        "id": "MPZFEGzSpagm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMsc7e4slD48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}