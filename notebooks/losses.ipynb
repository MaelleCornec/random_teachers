{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import AppLayout, GridspecLayout, IntSlider, FloatSlider, Button, Checkbox, Layout, HBox, VBox\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dinopl.tracking import FeatureSaver\n",
    "\n",
    "from torch.nn import functional as F\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_slider(name, range, start, update_fn, smooth=True, N=50):\n",
    "    SliderClass = FloatSlider if smooth else IntSlider\n",
    "    slider = SliderClass( description=name,\n",
    "                layout=Layout(height='auto', width='auto'),\n",
    "                orientation='horizontal',\n",
    "                min=range[0],\n",
    "                max=range[1],\n",
    "                step=(range[1]-range[0]) / N,\n",
    "                value=start)\n",
    "    slider.observe(update_fn, names='value')\n",
    "    return slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkbox(name, start, update_fn):\n",
    "    ckbox = Checkbox(description=name, value=start)\n",
    "    ckbox.observe(update_fn, names='value')\n",
    "    return ckbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common losses in self-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotsim(x_star:torch.Tensor, x:torch.Tensor):\n",
    "    return (x_star * x).sum(dim=-1)\n",
    "\n",
    "def cossim(x_star:torch.Tensor, x:torch.Tensor):\n",
    "    return dotsim(x_star, x) / (x_star.norm(dim=-1) * x.norm(dim=-1) + 1e-4)\n",
    "\n",
    "def mse(x_star:torch.Tensor, x:torch.Tensor): # acutally sum of squred errors\n",
    "    return (x_star - x).square().sum(dim=-1)\n",
    "    return dotsim(x_star - x, x_star - x)\n",
    "    return -2*dotsim(x_star, x) + x_star.norm().square() + x.norm().square()\n",
    "\n",
    "def l2(x_star:torch.Tensor, x:torch.Tensor):\n",
    "    return mse(x_star, x).sqrt()\n",
    "\n",
    "def ce(x_star:torch.Tensor, x:torch.Tensor):\n",
    "    targ = F.softmax(x_star, dim=-1)\n",
    "    log_pred = F.log_softmax(x, dim=-1)\n",
    "    return torch.sum(targ * -log_pred, dim=-1)\n",
    "\n",
    "def kl(x_star:torch.Tensor, x:torch.Tensor):\n",
    "    targ = F.softmax(x_star, dim=-1)\n",
    "    log_targ = F.log_softmax(x_star, dim=-1)\n",
    "    log_pred = F.log_softmax(x, dim=-1)\n",
    "    return torch.sum(targ * -(log_pred - log_targ), dim=-1)\n",
    "\n",
    "def entropy(x:torch.Tensor):\n",
    "    prob = F.softmax(x, dim=-1)\n",
    "    log_prob = F.log_softmax(x, dim=-1)\n",
    "    return torch.sum(prob * -log_prob, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, x_star=torch.Tensor([1,1]), lim=(0,2), N=100, cN=50, clim=(None, None), ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.figure().gca()\n",
    "    \n",
    "    x = torch.linspace(lim[0], lim[1], N)\n",
    "    y = torch.linspace(lim[0], lim[1], N)\n",
    "    M = torch.stack(torch.meshgrid(x, y, indexing='ij'), dim=-1)\n",
    "    z = loss(x_star, M)\n",
    "\n",
    "    p = ax.contourf(M[:,:,0], M[:,:,1], z, levels=cN, vmin=clim[0], vmax=clim[1], cmap='jet')\n",
    "    ax.arrow(0, 0, x_star[0], x_star[1], length_includes_head=True, width=0.01, head_width=0.2, color='k')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_title(loss.__name__)\n",
    "    plt.colorbar(p, ax=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = (-2, 2)\n",
    "x_star = torch.Tensor([1,-1])\n",
    "_, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(10, 5))\n",
    "plot_loss(dotsim, x_star, lim, ax=ax[0][0])\n",
    "plot_loss(cossim, x_star, lim, ax=ax[1][0])\n",
    "plot_loss(mse, x_star, lim, ax=ax[0][1])\n",
    "plot_loss(l2,  x_star, lim, ax=ax[1][1])\n",
    "plot_loss(ce,  x_star, lim, ax=ax[0][2])\n",
    "plot_loss(kl,  x_star, lim, ax=ax[1][2])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cross entropy grows in O(n) like the l2 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_growth(lim=(-10, 10), N=100, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.figure().gca()\n",
    "\n",
    "    t = torch.linspace(lim[0], lim[1], N).unsqueeze(1)\n",
    "    X = t * torch.Tensor([-1, 1]).repeat(100,1)\n",
    "\n",
    "    x_star = torch.Tensor([-1,1])\n",
    "    #ax.plot(t, mse(x_star, X), label='$mse(x, x^*)$')\n",
    "    ax.plot(t, l2(x_star, X), label='$l2(x, x^*)$')\n",
    "    ax.plot(t, kl(x_star, X), label='$kl(x, x^*)$')\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "plot_growth(lim=(-10, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy in logits space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy_logits(lim=(-2,2), N=100, cN=50, alpha=1, clim=(None, None), ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.figure().gca()\n",
    "    \n",
    "    x = torch.linspace(lim[0], lim[1], N)\n",
    "    y = torch.linspace(lim[0], lim[1], N)\n",
    "    M = torch.stack(torch.meshgrid(x, y, indexing='ij'), dim=-1)\n",
    "    H = entropy(M)\n",
    "\n",
    "    p = ax.contourf(M[:,:,0], M[:,:,1], H, levels=cN, vmin=clim[0], vmax=clim[1], alpha=alpha, cmap='jet')\n",
    "    plt.colorbar(p, ax=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy_probas(N=100, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.figure().gca()\n",
    "    \n",
    "    prob = torch.linspace(0, 1, N)\n",
    "    prob = torch.stack((prob, 1-prob), dim=-1)\n",
    "\n",
    "    H = torch.sum(prob * -torch.log(prob), dim=-1)\n",
    "    ax.plot(prob, H)\n",
    "    ax.set_title('Entropy of Probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(12, 5))\n",
    "plot_entropy_logits(lim=(-8,8), ax=ax[0])\n",
    "ax[0].set_title('Entropy of Logits')\n",
    "ax[0].set_aspect('equal', 'box')\n",
    "plot_entropy_probas(ax=ax[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The effect of sharpening on entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sharpening(t_min=0.2, lim=(-10, 10), N=100):\n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(12, 5))\n",
    "    plot_entropy_logits(lim=lim, N=N, cN=N//2, ax=ax[0])\n",
    "    ax[0].set_aspect('equal', 'box')\n",
    "    ax[0].set_title('Entropy of Logits')\n",
    "\n",
    "    # data\n",
    "    state=dict(r=1, phi=-.25*math.pi)\n",
    "    r, phi = state['r'], state['phi']\n",
    "    x = torch.Tensor([r*math.cos(phi), r*math.sin(phi)])\n",
    "    t = 1/torch.linspace(1, 1/t_min, N)\n",
    "    xs = x.unsqueeze(0)/t.unsqueeze(1)\n",
    "\n",
    "    # plot\n",
    "    arr1 = ax[0].arrow(0, 0, x[0], x[1], length_includes_head=True, width=0.01, head_width=0.2, color='k')\n",
    "    arr2 = ax[0].arrow(0, 0, x[0]/t_min, x[1]/t_min)\n",
    "    ax[0].set_xlim(lim)\n",
    "    ax[0].set_ylim(lim)\n",
    "\n",
    "    line, = ax[1].plot(t, entropy(xs))\n",
    "    ax[1].set_ylim((0, math.log(2)))\n",
    "    ax[1].invert_xaxis()\n",
    "    ax[1].set_title('Entropy vs Temperature')\n",
    "\n",
    "    def update(state):        \n",
    "        r, phi = state['r'], state['phi']\n",
    "        x = torch.Tensor([r*math.cos(phi), r*math.sin(phi)])\n",
    "        xs = x.unsqueeze(0)/t.unsqueeze(1)\n",
    "\n",
    "        arr1.set_data(x=0, y=0, dx=x[0], dy=x[1])\n",
    "        arr2.set_data(x=0, y=0, dx=x[0]/t_min, dy=x[1]/t_min)\n",
    "        line.set_ydata(entropy(xs))\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "    \n",
    "    def update_r(change):\n",
    "        state['r'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    def update_phi(change):\n",
    "        state['phi'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    sliders = VBox([\n",
    "        create_slider('r', range=(0, lim[1]), start=state['r'], update_fn=update_r),\n",
    "        create_slider('phi', range=(-1.25*math.pi, .75*math.pi), start=state['phi'], update_fn=update_phi)],\n",
    "        layout=Layout(width='40%', height='auto', margin = '0px 30% 0px 30%')\n",
    "    )\n",
    "\n",
    "    widget = AppLayout(\n",
    "        center=fig.canvas,\n",
    "        footer=sliders,\n",
    "        pane_heights=[0, 6, 1]\n",
    "    )\n",
    "    plt.ion()\n",
    "    return widget\n",
    "\n",
    "plot_sharpening(t_min=0.1, lim=(-10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = plt.figure().gca()\n",
    "#\n",
    "#n_classes = 2048\n",
    "#lim = (0, 4)\n",
    "#\n",
    "#x = torch.randn(n_classes).unsqueeze(0)\n",
    "#t = 10**-torch.linspace(lim[0], lim[1], n_classes).unsqueeze(1)\n",
    "#\n",
    "#H = -torch.sum(F.softmax(x/t, dim=-1)*F.log_softmax(x/t, dim=-1), dim=-1)\n",
    "#\n",
    "#ax.plot(t, H)\n",
    "#ax.invert_xaxis()\n",
    "#ax.set_xscale('log')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of centering and sharpening on crossentropy / kl divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kl_logits(lim=(-2, 2), N = 100, cN = 50):\n",
    "    # Widget should handly interactive \n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(8, 4))\n",
    "\n",
    "    # Data\n",
    "    state=dict(r=1, phi=-.5, t_stud=1)\n",
    "    r, phi, t_stud = state['r'], (state['phi'] + 0.25)*math.pi, state['t_stud']\n",
    "    x = torch.Tensor([r*math.cos(phi), r*math.sin(phi)])\n",
    "    M = torch.stack(torch.meshgrid([\n",
    "            torch.linspace(lim[0], lim[1], N),\n",
    "            torch.linspace(lim[0], lim[1], N)],\n",
    "            indexing='ij'), dim=-1)\n",
    "    KL = kl(x, M)\n",
    "\n",
    "    # Default axis\n",
    "    vmin, vmax = KL.min(), KL.max()\n",
    "    cont = ax.contourf(M[:,:,0]/t_stud, M[:,:,1]/t_stud, KL, levels=cN, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    arr1 = ax.arrow(0, 0, x[0], x[1], length_includes_head=True, width=0.01, head_width=0.2, color='k', zorder=10)\n",
    "\n",
    "\n",
    "    ax.set_xlim(lim)\n",
    "    ax.set_ylim(lim)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "    plt.colorbar(cont, ax=ax)\n",
    "    plt.suptitle('KL-Divergence of Logits')\n",
    "\n",
    "\n",
    "    def update(state):\n",
    "        r, phi, t_stud = state['r'], (state['phi'] + 0.25)*math.pi, state['t_stud']\n",
    "        x = torch.Tensor([r*math.cos(phi), r*math.sin(phi)])\n",
    "        KL_new = kl(x, M)\n",
    "\n",
    "        #for c in cont.collections:\n",
    "        #    ax.collections.remove(c)\n",
    "        #    cont.collections.remove(c)  # removes only the contours, leaves the rest intact\n",
    "        cont = ax.contourf(M[:,:,0]/t_stud, M[:,:,1]/t_stud, KL_new, levels=cN, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        arr1.set_data(x=0, y=0, dx=x[0], dy=x[1])\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "    def update_r(change):\n",
    "        state['r'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    def update_phi(change):\n",
    "        state['phi'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    def update_t_stud(change):\n",
    "        state['t_stud'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    sliders = VBox([\n",
    "        create_slider('r', range=(0, lim[1]), start=state['r'], update_fn=update_r),\n",
    "        create_slider('phi', range=(-1, 1), start=state['phi'], update_fn=update_phi),\n",
    "        create_slider('t_stud', range=(0.1, 1), start=state['t_stud'], update_fn=update_t_stud)],\n",
    "        layout=Layout(width='40%', height='auto', margin = '0px 20% 0px 20%')\n",
    "    )\n",
    "\n",
    "    widget = AppLayout(\n",
    "        center=fig.canvas,\n",
    "        footer=sliders,\n",
    "        pane_heights=[0, 8, 2]\n",
    "        )\n",
    "\n",
    "    plt.ion()\n",
    "    return widget\n",
    "\n",
    "plot_kl_logits(lim=(-2,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "R,_ = cv2.Rodrigues(np.pi * np.array([0.25, 0.25, 0.25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy_logits_3d(lim=(-2,2), N=100, cN=50, clim=(None, None), ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.figure().gca()\n",
    "    \n",
    "    x = torch.linspace(lim[0], lim[1], N)\n",
    "    y = torch.linspace(lim[0], lim[1], N)\n",
    "    X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "    Z = torch.zeros_like(X)\n",
    "    M = torch.stack((X, Y, Z), dim=-1)\n",
    "\n",
    "    v_from, v_to = torch.Tensor([0,0,1]), F.normalize(torch.Tensor([1, 1, 1]), dim=0)\n",
    "    R = torch.from_numpy(cv2.Rodrigues(torch.cross(v_from, v_to).numpy())[0])\n",
    "    H = entropy(M @ R.T)\n",
    "\n",
    "    p = ax.contourf(M[:,:,0], M[:,:,1], H, levels=cN, vmin=clim[0], vmax=clim[1], cmap='jet')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_title('Entropy of Logits 3D')\n",
    "    plt.colorbar(p, ax=ax)\n",
    "    return ax\n",
    "plot_entropy_logits_3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sharpening_3d(t_min=0.2, lim=(-10, 10), N=100):\n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(12, 5))\n",
    "    plot_entropy_logits_3d(lim=lim, N=N, cN=N//2, ax=ax[0])\n",
    "\n",
    "    v_from, v_to = torch.Tensor([0,0,1]), F.normalize(torch.Tensor([1, 1, 1]), dim=0)\n",
    "    R = torch.from_numpy(cv2.Rodrigues(torch.cross(v_from, v_to).numpy())[0])\n",
    "\n",
    "    # data\n",
    "    state=dict(r=1, phi=-.25*math.pi)\n",
    "    r, phi = state['r'], state['phi']\n",
    "    x = R @ torch.Tensor([r*math.cos(phi), r*math.sin(phi), 0])\n",
    "    t = 1/torch.linspace(1, 1/t_min, N)\n",
    "    xs = x.unsqueeze(0)/t.unsqueeze(1)\n",
    "\n",
    "    \n",
    "    \n",
    "    # plot\n",
    "    arr1 = ax[0].arrow(0, 0, x[0], x[1], length_includes_head=True, width=0.01, head_width=0.2, color='k')\n",
    "    arr2 = ax[0].arrow(0, 0, x[0]/t_min, x[1]/t_min)\n",
    "    ax[0].set_xlim(lim)\n",
    "    ax[0].set_ylim(lim)\n",
    "\n",
    "    line, = ax[1].plot(t, entropy(xs))\n",
    "    ax[1].set_ylim((0, math.log(3)))\n",
    "    ax[1].invert_xaxis()\n",
    "    ax[1].set_title('Entropy vs Temperature')\n",
    "\n",
    "    def update(state):        \n",
    "        r, phi = state['r'], state['phi']\n",
    "        x = R @ torch.Tensor([r*math.cos(phi), r*math.sin(phi), 0])\n",
    "        xs = x.unsqueeze(0)/t.unsqueeze(1)\n",
    "\n",
    "        arr1.set_data(x=0, y=0, dx=x[0], dy=x[1])\n",
    "        arr2.set_data(x=0, y=0, dx=x[0]/t_min, dy=x[1]/t_min)\n",
    "        line.set_ydata(entropy(xs))\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "    \n",
    "    def update_r(change):\n",
    "        state['r'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    def update_phi(change):\n",
    "        state['phi'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    sliders = VBox([\n",
    "        create_slider('r', range=(0, lim[1]), start=state['r'], update_fn=update_r),\n",
    "        create_slider('phi', range=(-1.25*math.pi, .75*math.pi), start=state['phi'], update_fn=update_phi)],\n",
    "        layout=Layout(width='40%', height='auto', margin = '0px 30% 0px 30%')\n",
    "    )\n",
    "\n",
    "    widget = AppLayout(\n",
    "        center=fig.canvas,\n",
    "        footer=sliders,\n",
    "        pane_heights=[0, 6, 1]\n",
    "    )\n",
    "    plt.ion()\n",
    "    return widget\n",
    "\n",
    "plot_sharpening_3d(t_min=0.1, lim=(-10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kl_logits_3d(lim=(-2, 2), N = 100, cN = 50):\n",
    "    # Widget should handly interactive \n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(8, 4))\n",
    "\n",
    "    v_from, v_to = torch.Tensor([0,0,1]), torch.Tensor([1, 1, 1]) / math.sqrt(3)\n",
    "    R = torch.from_numpy(cv2.Rodrigues(torch.cross(v_from, v_to).numpy())[0])\n",
    "\n",
    "    # Data\n",
    "    state=dict(r=1, phi=-.5, t_stud=1)\n",
    "    r, phi, t_stud = state['r'], (state['phi'] + 0.25)*math.pi, state['t_stud']\n",
    "    x = torch.Tensor([r*math.cos(phi), r*math.sin(phi), 0])\n",
    "\n",
    "    X, Y =torch.meshgrid([\n",
    "            torch.linspace(lim[0], lim[1], N),\n",
    "            torch.linspace(lim[0], lim[1], N)],\n",
    "            indexing='ij')\n",
    "    Z = torch.zeros_like(X)\n",
    "    M = torch.stack((X, Y, Z), dim=-1)\n",
    "\n",
    "    KL = kl(R @ x, M @ R.T)\n",
    "\n",
    "    # Default axis\n",
    "    vmin, vmax = KL.min(), KL.max()\n",
    "    cont = ax.contourf(M[:,:,0]/t_stud, M[:,:,1]/t_stud, KL, levels=cN, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    arr1 = ax.arrow(0, 0, x[0], x[1], length_includes_head=True, width=0.01, head_width=0.2, color='k', zorder=10)\n",
    "\n",
    "\n",
    "    ax.set_xlim(lim)\n",
    "    ax.set_ylim(lim)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "    plt.colorbar(cont, ax=ax)\n",
    "    plt.suptitle('KL-Divergence of Logits')\n",
    "\n",
    "\n",
    "    def update(state):\n",
    "        r, phi, t_stud = state['r'], (state['phi'] + 0.25)*math.pi, state['t_stud']\n",
    "        x = torch.Tensor([r*math.cos(phi), r*math.sin(phi), 0])\n",
    "        KL_new = kl(R @ x, M @ R.T)\n",
    "\n",
    "        #for c in cont.collections:\n",
    "        #    ax.collections.remove(c)\n",
    "        #    cont.collections.remove(c)  # removes only the contours, leaves the rest intact\n",
    "        cont = ax.contourf(M[:,:,0]/t_stud, M[:,:,1]/t_stud, KL_new, levels=cN, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        arr1.set_data(x=0, y=0, dx=x[0], dy=x[1])\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "    def update_r(change):\n",
    "        state['r'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    def update_phi(change):\n",
    "        state['phi'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    def update_t_stud(change):\n",
    "        state['t_stud'] = change.new\n",
    "        update(state)\n",
    "\n",
    "    sliders = VBox([\n",
    "        create_slider('r', range=(0, lim[1]), start=state['r'], update_fn=update_r),\n",
    "        create_slider('phi', range=(-1, 1), start=state['phi'], update_fn=update_phi),\n",
    "        create_slider('t_stud', range=(0.1, 1), start=state['t_stud'], update_fn=update_t_stud)],\n",
    "        layout=Layout(width='40%', height='auto', margin = '0px 20% 0px 20%')\n",
    "    )\n",
    "\n",
    "    widget = AppLayout(\n",
    "        center=fig.canvas,\n",
    "        footer=sliders,\n",
    "        pane_heights=[0, 8, 2]\n",
    "        )\n",
    "\n",
    "    plt.ion()\n",
    "    return widget\n",
    "\n",
    "plot_kl_logits_3d(lim=(-10,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proje, names, indices = FeatureSaver.load_data(os.path.join(os.environ['DINO_RESULTS'],'wandb/offline-run-20220803_183004-2wkk2y2l/files/valid/feat/proje'))\n",
    "data_logit, names, indices = FeatureSaver.load_data(os.path.join(os.environ['DINO_RESULTS'],'wandb/offline-run-20220803_183004-2wkk2y2l/files/valid/feat/logit'))\n",
    "print(names)\n",
    "print(data_proje.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamics in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamics(data_proje, data_logit):\n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(1,2, sharex=False, sharey=False, figsize=(14, 7))\n",
    "    lbls = data_proje[0,:,0].int().tolist()\n",
    "\n",
    "    state = dict(step=0, arrows=True)\n",
    "    step = state['step']\n",
    "\n",
    "    proje = dict(name='proje', data=data_proje, ax=ax[0])\n",
    "    logit = dict(name='logit', data=data_logit, ax=ax[1])\n",
    "    for feat in [proje, logit]:\n",
    "        \n",
    "        data = feat['data']\n",
    "        ax = feat['ax']\n",
    "\n",
    "        lim = data[:,:,1:].min(), data[:,:,1:].max()\n",
    "        print(lim)\n",
    "        plot_entropy_logits(lim, alpha=0.5, ax=ax)\n",
    "\n",
    "        feat['scat_t'] = ax.scatter(data[step,:,1], data[step,:,2], s=10, c=lbls, label=lbls, marker='+') # teacher\n",
    "        feat['scat_s'] = ax.scatter(data[step,:,3], data[step,:,4], s=10, c=lbls, label=lbls, marker='o') # student\n",
    "        \n",
    "        quiv_kwargs = dict(units='xy', scale=1, width=0.1, zorder=10, visible=(state['arrows'] and feat['name'] == 'logit'))\n",
    "        feat['quiv'] = ax.quiver(data[step,:,3], # X\n",
    "                        data[step,:,4], # Y\n",
    "                        data[step,:,1] - data[step,:,3], # U\n",
    "                        data[step,:,2] - data[step,:,4], # V\n",
    "                        lbls, #C\n",
    "                        **quiv_kwargs) # V\n",
    "\n",
    "        feat['ax'].set_xlim(lim)\n",
    "        feat['ax'].set_ylim(lim)\n",
    "        feat['ax'].set_aspect('equal', 'box')\n",
    "        \n",
    "    #plt.colorbar(scat_t, ax=ax)\n",
    "\n",
    "\n",
    "    def update_step(change):\n",
    "        state['step'] = change.new\n",
    "        step = state['step']\n",
    "\n",
    "        for feat in [proje, logit]:\n",
    "            data = feat['data']\n",
    "            feat['scat_t'].set_offsets(data[step,:,1:3])\n",
    "            feat['scat_s'].set_offsets(data[step,:,3:5])\n",
    "            feat['quiv'].set_offsets(data[step,:,3:5])\n",
    "            feat['quiv'].set_UVC(data[step,:,1] - data[step,:,3], data[step,:,2] - data[step,:,4])\n",
    "\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "    \n",
    "    def update_arr_visibility(change):\n",
    "        state['arrows'] = change.new\n",
    "        logit['quiv'].set(visible=state['arrows'])\n",
    "\n",
    "    sliders = VBox([\n",
    "        create_slider('step', range=(0, data.shape[0]), start=state['step'], N=data.shape[0], smooth=False, update_fn=update_step),\n",
    "        create_checkbox('arrows', start=state['arrows'], update_fn=update_arr_visibility),\n",
    "        ],\n",
    "        layout=Layout(width='40%', height='auto', margin = '0px 20% 0px 20%')\n",
    "    )\n",
    "\n",
    "    widget = AppLayout(\n",
    "        center=fig.canvas,\n",
    "        footer=sliders,\n",
    "        pane_heights=[0, 8, 2]\n",
    "        )        \n",
    "\n",
    "    plt.ion()\n",
    "    return widget\n",
    "plot_dynamics(data_proje=data_proje, data_logit=data_logit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamics in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamics with n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join(os.environ['DINO_RESULTS'],'wandb/run-20220804_212135-anpd7oxy/files/valid/feat/logit')\n",
    "data_logit, names, indices = FeatureSaver.load_data(dir, start=0, stop=-1, step=10)\n",
    "print(names[:5], '...', names[-5:])\n",
    "print(data_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = data_logit[:,:,0]\n",
    "t_logit = data_logit[:,:,1:1025]\n",
    "s_logit = data_logit[:,:,1025:2049]\n",
    "t_proba = F.softmax(t_logit, dim=-1)\n",
    "s_proba = F.softmax(s_logit, dim=-1)\n",
    "lbls.shape, t_proba.shape, s_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamics_probas(lbls, t_proba, s_proba):\n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(1,2, sharex=True, sharey=True, figsize=(10, 5))\n",
    "\n",
    "    n_steps, n_samples, n_classes = t_proba.shape\n",
    "    state = dict(step=0, sample=0)\n",
    "    step = state['step']\n",
    "    sample = state['sample']\n",
    "\n",
    "    t_bar = ax[0].bar(torch.arange(0, n_classes), t_proba[step, sample, :], width=2)\n",
    "    ax[0].set_title(f'teacher-produced labels')\n",
    "    ax[0].set_ylim(0,1)\n",
    "\n",
    "    s_bar = ax[1].bar(torch.arange(0, n_classes), s_proba[step, sample, :], width=2)\n",
    "    ax[1].set_title(f'student predictions')\n",
    "    ax[1].set_ylim(0,1)\n",
    "\n",
    "    title = plt.suptitle(f'Label Dynamics of Image {sample} with groundruth {lbls[0, sample]}')\n",
    "\n",
    "    \n",
    "    def update(step, sample):\n",
    "        for i, (t_rect, s_rect) in enumerate(zip(t_bar, s_bar)):\n",
    "            t_rect.set_height(t_proba[step, sample, i])\n",
    "            s_rect.set_height(s_proba[step, sample, i])\n",
    "\n",
    "        title.set_text(f'Label Dynamics of Image {sample} with groundruth {lbls[0, sample]}')\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "\n",
    "    def update_step(change):\n",
    "        state['step'] = change.new\n",
    "        update(state['step'], state['sample'])\n",
    "\n",
    "    def update_sample(change):\n",
    "        state['sample'] = change.new\n",
    "        update(state['step'], state['sample'])\n",
    "\n",
    "    sliders = VBox([\n",
    "        create_slider('step', range=(0, n_steps-1), start=state['step'], N=n_steps-1, smooth=False, update_fn=update_step),\n",
    "        create_slider('sample', range=(0, n_samples-1), start=state['sample'], N=n_samples-1, smooth=False, update_fn=update_sample),\n",
    "        ],\n",
    "        layout=Layout(width='40%', height='auto', margin = '0px 20% 0px 20%')\n",
    "    )\n",
    "\n",
    "    widget = AppLayout(\n",
    "        center=fig.canvas,\n",
    "        footer=sliders,\n",
    "        pane_heights=[0, 8, 2]\n",
    "        )        \n",
    "\n",
    "    plt.ion()\n",
    "    return widget\n",
    "\n",
    "plot_dynamics_probas(lbls, t_proba, s_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairwise_dist(data, dist_fn, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.figure().gca()\n",
    "\n",
    "    data = data.squeeze()\n",
    "    n_samples = data.shape[0]\n",
    "\n",
    "    arg1 = data.unsqueeze(0).expand(n_samples, -1, -1)\n",
    "    arg2 = data.unsqueeze(1).expand(-1, n_samples, -1)\n",
    "    \n",
    "    mat = dist_fn(arg1, arg2)\n",
    "    im = ax.imshow(mat)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = -1\n",
    "_, ax = plt.subplots(2,2, figsize=(10, 8))\n",
    "\n",
    "ax[0][0].set_title('cossim of teacher')\n",
    "plot_pairwise_dist(t_logit[step, lbls[0].argsort(), :], cossim, ax=ax[0][0])\n",
    "\n",
    "ax[0][1].set_title('cossim of student')\n",
    "plot_pairwise_dist(s_logit[step, lbls[0].argsort(), :], cossim, ax=ax[0][1])\n",
    "\n",
    "ax[1][0].set_title('kldiv of teacher')\n",
    "plot_pairwise_dist(t_logit[step, lbls[0].argsort(), :], kl, ax=ax[1][0])\n",
    "\n",
    "ax[1][1].set_title('kldiv of student')\n",
    "plot_pairwise_dist(s_logit[step, lbls[0].argsort(), :], kl, ax=ax[1][1])\n",
    "\n",
    "plt.suptitle(f'Within-dataset pairwise distances at step {step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, stop, step = 0, -1, 100\n",
    "\n",
    "n_steps, n_samples, n_classes = t_logit.shape\n",
    "dim = n_samples * n_classes\n",
    "\n",
    "_, ax = plt.subplots(2,2, figsize=(10, 8))\n",
    "\n",
    "ax[0][0].set_title('cossim of teacher')\n",
    "plot_pairwise_dist(t_logit[start:stop:step].reshape(-1, dim), cossim, ax=ax[0][0])\n",
    "\n",
    "ax[0][1].set_title('cossim of student')\n",
    "plot_pairwise_dist(s_logit[start:stop:step].reshape(-1, dim), cossim, ax=ax[0][1])\n",
    "\n",
    "ax[1][0].set_title('kldiv of teacher')\n",
    "plot_pairwise_dist(t_logit[start:stop:step].reshape(-1, dim), kl, ax=ax[1][0])\n",
    "\n",
    "ax[1][1].set_title('kldiv of student')\n",
    "plot_pairwise_dist(s_logit[start:stop:step].reshape(-1, dim), kl, ax=ax[1][1])\n",
    "\n",
    "plt.suptitle(f'Training dynamic of function space over validation images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Denoising Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targs = t_logit[-1].transpose(-1, -2)\n",
    "t_preds = t_logit.transpose(-1, -2)\n",
    "s_preds = s_logit.transpose(-1, -2)\n",
    "\n",
    "#plt.plot(kl(s_logit.reshape(-1, dim), s_logit[-1].reshape(-1)))        # track kl wrt. to final label averaged over images and classes\n",
    "#plt.plot(kl(s_logit, s_logit[-1]))                                     # for every validation image => track kl wrt to final label\n",
    "\n",
    "_, ax = plt.subplots(2, 1, sharex=True, sharey=False, figsize=(12,6))\n",
    "ax[0].set_title('teacher predictions vs final pseudolabels')\n",
    "ax[0].plot(kl(targs, t_preds), linewidth=1)  # for every pseudo-class => track kl wrt to final label\n",
    "\n",
    "ax[1].set_title('student predictions vs final pseudolabels')\n",
    "ax[1].plot(kl(targs, s_preds), linewidth=1)  # for every pseudo-class => track kl wrt to final label\n",
    "plt.suptitle('Class-wise Denoising: KL-Divergence wrt. final pseudolabels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 1, sharex=True, sharey=False, figsize=(12,6))\n",
    "ax[0].set_title('teacher predictions vs final pseudolabels')\n",
    "t_peaks = kl(targs, t_preds).max(dim=0)\n",
    "ax[0].scatter(t_peaks.indices, t_peaks.values, s=1)  # for every pseudo-class => track kl wrt to final label\n",
    "ax[0].set_xlim(0, len(t_preds))\n",
    "\n",
    "ax[1].set_title('student predictions vs final pseudolabels')\n",
    "s_peaks = kl(targs, s_preds).max(dim=0)\n",
    "ax[1].scatter(s_peaks.indices, s_peaks.values, s=1)  # for every pseudo-class => track kl wrt to final label\n",
    "ax[1].set_xlim(0, len(s_preds))\n",
    "\n",
    "plt.suptitle('Class-wise Denoising: KL-Divergence peaks during training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('dino')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c88d231864a4bea7e4388fcc609bd62bc116121bdaee26c7f9d55d94cdceaa51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
