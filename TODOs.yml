TODOs:
- reproduce original dino exactly
- freeze last layer during first epoch
- losses => micro vs macro?
- Speedup? DL took 2h? -> now reached that?!

- probing/logging:
- Cache Embeddings for Evaluator? use dino dataloader for embedding generation, then second dataloader for embeddings
- auto_gpu => dataloader after trainer definition
- Online Evaluator for comparison? 
- Linear Solve or KNN evaluator?

- experiment setup:
  - create logging name and directory
  - integrate with wandb
  - store config to wandb
- schedule: 
  - copy for parameter location?
  - Schedule.__repr__(self, ) instead of super(CatSched, self)

- postponed:
- print CE, KL, H_targ to console 

