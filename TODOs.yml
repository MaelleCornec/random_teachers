Runs/Questions:
- dimensional collapse with out_dim=1024?
- store features and check if distillation is happening?
- train with augmentations but without teacher update?
- better metric to evaluate if it works? within class correlation? vs between class correlation? => linear separability?
- what if we train to minimize entropy with l2bottleneck? 

# only global crops 28x28, mnist => works perfectly (tiny input seemed to be the the problem previously)
bsub -n 4 -W 24:00 -R "rusage[mem=4096, ngpus_excl_p=1]" -R "select[gpu_model0==NVIDIAGeForceRTX2080Ti]" python dino.py --mc 2x28 --tiny_input

# cifar10 => works to some degree.. 
bsub -n 4 -W 24:00 -R "rusage[mem=4096, ngpus_excl_p=1]" -R "select[gpu_model0==NVIDIAGeForceRTX2080Ti]" python dino.py --dataset cifar10 --mc 2x32 --tiny_input
bsub -n 4 -W 24:00 -R "rusage[mem=4096, ngpus_excl_p=1]" -R "select[gpu_model0==NVIDIAGeForceRTX2080Ti]" python dino.py --dataset cifar10

# simplify optimization parameters => crashed because of sgd
bsub -n 4 -W 24:00 -R "rusage[mem=4096, ngpus_excl_p=1]" python dino.py --mc 2x28 --tiny_input --out_dim 1024 --t_mom 0.996 --opt sgd --opt_lr 0.0005 --opt_wd 0.04 --wn_freeze_epochs 0

# save features & rank collapse? => sharpening produces higher rank
bsub -n 4 -W 24:00 -R "rusage[mem=4096, ngpus_excl_p=1]" python dino.py --mc 2x28+4x28 --tiny_input --out_dim 1024 --save_features all

# only augmentations no update => works.. but why?
bsub -n 4 -W 24:00 -R "rusage[mem=4096, ngpus_excl_p=1]" python dino.py --mc 2x28+4x28 --tiny_input --out_dim 1024 --t_mode no_update

# only optimize H_preds => rank collapses & accuracy decreases..
bsub -n 4 -W 24:00 -R "rusage[mem=4096, ngpus_excl_p=1]" python dino.py --mc 2x28 --tiny_input --out_dim 1024 --loss H_pred --s_cmom 0

Discussion of Results:
- Why doesn't it perform well on CIFAR? Is there an implementation bug? Is longer training required? Are color augmentations needed for CIFAR? Do less augmentations cause higher dimensional collapse?
- Why is the embedding rank limited to 129 for mc 28x28? Is there a problem with the encoder or is this normal?
- Why does the network learn good features even if the teacher is not updated?

Further Work:
- MNIST; use simpler convnet than resnet, e.g. LeNet?
- Simplify schedules, optimizer sgd
- Do we need augmentations at all if networks are far appart? Does the loss with no augmentations encourage convergence of networks? 
- Compare to label denoising literature.. 

Theory:
- What is a good embedding? Quality of linear classifier? Linear separability on down stream task? Similarity to supervised embeddings?
- Why is high rank solution preferable? => collapse limits solutions for linear classification because of VC dimension. 

Algorithmic Ambiquities:
- How should teacher buffers be updated? update BN buffers from student ema update or indepentently at training time?
- How should the losses be aggregated? First over batch, then over crops? micro vs macro?

Implementation TODOs:
- make sure that training mode is correct everywhere => should teacher be in eval or train? what happens to BN buffers during ema?
- SGD does not take initial lr parameter => needs to be set at initialization
- Speedup? DL took 2h? -> now reached that?! => profiling?
- Original DINO converged to different loss at some point? Are results reproducible with orignal implementation and fastai?
  - track stats of embs and logits in original DINO?
  - phil het gseit => t_mom schedule float64?
- Why does autoselect_gpu not work?

Testbed / Experimentation TODOS:
- create logging name and directory
- integrate with wandb => what else?
- Online probing for live measurement of emedding quality? 
- KNN probing for faster and more robust measurement of embedding quality?